#Need to work on RBAC
---
# Source: spark-operator/templates/spark-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark-operator-spark
  namespace: tenant1
  labels:
    helm.sh/chart: spark-operator-1.1.27
    app.kubernetes.io/name: spark-operator
    app.kubernetes.io/instance: spark-operator
    app.kubernetes.io/version: "v1beta2-1.3.8-3.1.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: spark-operator/templates/spark-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: spark-role
  namespace: tenant1
  labels:
    helm.sh/chart: spark-operator-1.1.27
    app.kubernetes.io/name: spark-operator
    app.kubernetes.io/instance: spark-operator
    app.kubernetes.io/version: "v1beta2-1.3.8-3.1.1"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - "*"
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - "*"
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - "*"
- apiGroups:
  - ""
  resources:
  - persistentvolumeclaims
  verbs:
  - "*"
---
# Source: spark-operator/templates/spark-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: spark
  namespace: tenant1
  labels:
    helm.sh/chart: spark-operator-1.1.27
    app.kubernetes.io/name: spark-operator
    app.kubernetes.io/instance: spark-operator
    app.kubernetes.io/version: "v1beta2-1.3.8-3.1.1"
    app.kubernetes.io/managed-by: Helm
subjects:
- kind: ServiceAccount
  name: spark-operator-spark
  namespace: tenant1
roleRef:
  kind: Role
  name: spark-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: spark-operator/templates/deployment.yaml
# If the admission webhook is enabled, then a post-install step is required
# to generate and install the secret in the operator namespace.

# In the post-install hook, the token corresponding to the operator service account
# is used to authenticate with the Kubernetes API server to install the secret bundle.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-operator
  labels:
    helm.sh/chart: spark-operator-1.1.27
    app.kubernetes.io/name: spark-operator
    app.kubernetes.io/instance: spark-operator
    app.kubernetes.io/version: "v1beta2-1.3.8-3.1.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: spark-operator
      app.kubernetes.io/instance: spark-operator
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "10254"
        prometheus.io/path: /metrics
      labels:
        app.kubernetes.io/name: spark-operator
        app.kubernetes.io/instance: spark-operator
    spec:
      serviceAccountName: spark-operator
      securityContext:
        {}
      containers:
      - name: spark-operator
        image: ghcr.io/googlecloudplatform/spark-operator:v1beta2-1.3.8-3.1.1
        imagePullPolicy: IfNotPresent
        securityContext:
          {}
        ports:
          - name: "metrics"
            containerPort: 10254
        
        args:
        - -v=2
        - -logtostderr
        - -namespace=
        - -enable-ui-service=true
        - -ingress-url-format=
        - -controller-threads=10
        - -resync-interval=30
        - -enable-batch-scheduler=false
        - -label-selector-filter=
        - -enable-metrics=true
        - -metrics-labels=app_type
        - -metrics-port=10254
        - -metrics-endpoint=/metrics
        - -metrics-prefix=
        - -enable-resource-quota-enforcement=false
        resources:
          {}
---
# Source: spark-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark-operator
  annotations:
    "helm.sh/hook": pre-install, pre-upgrade
    "helm.sh/hook-delete-policy": hook-failed, before-hook-creation
    "helm.sh/hook-weight": "-10"
  labels:
    helm.sh/chart: spark-operator-1.1.27
    app.kubernetes.io/name: spark-operator
    app.kubernetes.io/instance: spark-operator
    app.kubernetes.io/version: "v1beta2-1.3.8-3.1.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: spark-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: spark-operator
  annotations:
    "helm.sh/hook": pre-install, pre-upgrade
    "helm.sh/hook-delete-policy": hook-failed, before-hook-creation
    "helm.sh/hook-weight": "-10"
  labels:
    helm.sh/chart: spark-operator-1.1.27
    app.kubernetes.io/name: spark-operator
    app.kubernetes.io/instance: spark-operator
    app.kubernetes.io/version: "v1beta2-1.3.8-3.1.1"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - "*"
- apiGroups:
  - ""
  resources:
  - services
  - configmaps
  - secrets
  verbs:
  - create
  - get
  - delete
  - update
- apiGroups:
  - extensions
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - create
  - get
  - delete
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - update
  - patch
- apiGroups:
  - ""
  resources:
  - resourcequotas
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - create
  - get
  - update
  - delete
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - mutatingwebhookconfigurations
  - validatingwebhookconfigurations
  verbs:
  - create
  - get
  - update
  - delete
- apiGroups:
  - sparkoperator.k8s.io
  resources:
  - sparkapplications
  - sparkapplications/status
  - scheduledsparkapplications
  - scheduledsparkapplications/status
  verbs:
  - "*"
---
# Source: spark-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: spark-operator
  annotations:
    "helm.sh/hook": pre-install, pre-upgrade
    "helm.sh/hook-delete-policy": hook-failed, before-hook-creation
    "helm.sh/hook-weight": "-10"
  labels:
    helm.sh/chart: spark-operator-1.1.27
    app.kubernetes.io/name: spark-operator
    app.kubernetes.io/instance: spark-operator
    app.kubernetes.io/version: "v1beta2-1.3.8-3.1.1"
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: spark-operator
    namespace: tenant1
roleRef:
  kind: ClusterRole
  name: spark-operator
  apiGroup: rbac.authorization.k8s.io

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-conf
  namespace: tenant1
data:
  spark-defaults.conf: |
    spark.kubernetes.container.image=eragani/spark:3.3.1

---
---
# ServiceAccount for spark in tenant1 namespace
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark
  namespace: tenant1
  labels:
    app: spark

---
# Role defining permissions for spark in tenant1 namespace
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: spark-role
  namespace: tenant1
  labels:
    app: spark
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
# RoleBinding for spark and Argo CD application controller in tenant1 namespace
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: spark-role-binding
  namespace: tenant1
  labels:
    app: spark
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: spark-role
subjects:
- kind: ServiceAccount
  name: spark
  namespace: tenant1
- kind: ServiceAccount
  name: argocd-application-controller  
  namespace: argocd

---
# ClusterRole defining permissions for Argo CD across the cluster
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: argocd-cluster-role
  labels:
    app: argocd
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
# ClusterRoleBinding granting Argo CD the defined ClusterRole permissions
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: argocd-cluster-role-binding
  labels:
    app: argocd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: argocd-cluster-role
subjects:
- kind: ServiceAccount
  name: argocd-application-controller
  namespace: argocd

---

apiVersion: v1
kind: Secret
metadata:
  name: docker-secret
  namespace: tenant1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJkb2NrZXIuaW8iOnsidXNlcm5hbWUiOiJlcmFnYW5pIiwicGFzc3dvcmQiOiJBbnVyYWRoYUAxMjM0IyQiLCJlbWFpbCI6ImVyYWdhbmlAZ21haWwuY29tIiwiYXV0aCI6IlpYSmhaMkZ1YVRwQmJuVnlZV1JvWVVBeE1qTTBJeVE9In19fQ==
type: kubernetes.io/dockerconfigjson

---

apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: ScheduledSparkApplication
metadata:
  name: my-spark-app-tenant1
  namespace: tenant1
spec:
  schedule: "*/3 * * * *"
  concurrencyPolicy: Forbid
  template:
    type: Scala
    mode: cluster
    image: "eragani/spark:3.3.1"
    mainClass: "step1.UserCDCEventProcessor"
    mainApplicationFile: "local:///opt/suite/audience2.0/Audience-assembly-0.1.0.jar"
    sparkVersion: "3.3.1"
    restartPolicy:
      type: Never
    driver:
      cores: 1
      coreLimit: "1200m"
      memory: "512m"
      labels:
        version: 3.3.1
      serviceAccount: spark
    executor:
      cores: 1
      instances: 5
      memory: "512m"
      labels:
        version: 3.3.1
    sparkConf:
      "spark.kubernetes.namespace": "tenant1"
      "spark.kubernetes.authenticate.driver.serviceAccountName": "spark"
      "spark.kubernetes.container.image": "eragani/spark:3.3.1"
      "spark.kubernetes.driver.container.image": "eragani/spark:3.3.1"
      "spark.kubernetes.executor.container.image": "eragani/spark:3.3.1"
      "spark.eventLog.enabled": "false"
      "spark.kubernetes.container.image.pullSecrets": "docker-secret"
    arguments:
      - "au03_sales_test"

